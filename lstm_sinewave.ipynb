{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GFfxqMRAuMJ"
   },
   "source": [
    "This is a simple LSTM test with the goal to train to predict a sine wave using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5350,
     "status": "ok",
     "timestamp": 1690606128388,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "FqjSmhKkAuMK"
   },
   "outputs": [],
   "source": [
    "# Created by Davide Pasca - 2023/07/24\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34SKvAQ5ntrQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1690606128388,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "rCncPFAtAuML",
    "outputId": "327f621e-647a-45f3-e318-dc9cd8c3947f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gg31L9qBAuML"
   },
   "source": [
    "We use actual time values to have a realistic view of the number of samples involved if this were about predicting market data.\n",
    "Of course, the actual price data would be much different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1690606128388,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "MAUdr_F-AuMM"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "SAMPLE_SIZE_MINS = 60 * 6  # 6 hours\n",
    "\n",
    "USE_LOG_RETURNS = True\n",
    "\n",
    "NORMALIZE_TYPE = 'none'  # 'minmax', 'std', 'constant', 'none'\n",
    "\n",
    "# With log-returns, values tend to be very small\n",
    "# This is a reasonable scale\n",
    "CONST_NORM_SCALE = 15.0 if USE_LOG_RETURNS else 1.0\n",
    "\n",
    "LSTM_INPUT_SIZE = 1  # Only 1 feature (univariate time series data)\n",
    "LSTM_SEQUENCE_LENGTH = SAMPLE_SIZE_MINS * 6 * 2 // 60\n",
    "LSTM_LAYERS_N = 1\n",
    "# https://www.quora.com/How-should-I-set-the-size-of-hidden-state-vector-in-LSTM-in-keras/answer/Yugandhar-Nanda\n",
    "LSTM_HIDDEN_SIZE = LSTM_SEQUENCE_LENGTH // 1\n",
    "\n",
    "LEARNING_RATE_ADAM = 0.005\n",
    "LEARNING_RATE_ADAGRAD = 0.01\n",
    "LEARNING_RATE_RMS_PROP = 0.0001\n",
    "LEARNING_RATE_SGD = 0.01\n",
    "LEARNING_RATE_RADAM = 0.001\n",
    "LEARNING_RATE_ADAMW = 0.002\n",
    "\n",
    "L1_REGULARIZATION = 0*0.0001  #\n",
    "\n",
    "L2_WEIGHT_DECAY = 0.0 # 0.0001\n",
    "\n",
    "DROPOUT_P = 0.0  # 0 to disable\n",
    "\n",
    "EPOCHS_N = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1690606128388,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "uFh-bJs4AuMM"
   },
   "outputs": [],
   "source": [
    "def normalizeVal(val):\n",
    "    return val * NORMALIZE_SCALE + 0.0\n",
    "\n",
    "def denormalizeVal(val):\n",
    "    return (val - 0.0) / NORMALIZE_SCALE\n",
    "\n",
    "def generate_sine_wave(samplesN: int, frequency: float, minVal: float, maxVal: float) -> torch.Tensor:\n",
    "    amp = (maxVal - minVal) / 2.0\n",
    "    wave = [float(amp * (math.sin(2.0 * math.pi * frequency * i) + 1.0) + minVal) for i in range(samplesN)]\n",
    "    return torch.tensor(wave)\n",
    "\n",
    "def safeLog(vals: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.where(vals > 0, torch.log(vals), torch.tensor(0.0))\n",
    "\n",
    "def make_input_data(vals: torch.Tensor) -> Tuple[torch.Tensor, float, float]:\n",
    "    data = torch.empty_like(vals)  # create an empty tensor of the same size\n",
    "\n",
    "    if USE_LOG_RETURNS:\n",
    "        data[1:] = safeLog(vals[1:] / vals[:-1])\n",
    "        data[0] = data[1]  # copy the second element to the first\n",
    "    else:\n",
    "        data = vals\n",
    "\n",
    "    norm_off = 0.0\n",
    "    norm_sca = 0.0\n",
    "    if NORMALIZE_TYPE == 'std':\n",
    "        norm_off = data.mean()\n",
    "        norm_sca = data.std()\n",
    "    elif NORMALIZE_TYPE == 'minmax':\n",
    "        norm_off = data.min()\n",
    "        norm_sca = data.max() - data.min()\n",
    "    elif NORMALIZE_TYPE == 'constant':\n",
    "        norm_off = 0.0\n",
    "        norm_sca = CONST_NORM_SCALE\n",
    "    elif NORMALIZE_TYPE == 'none':\n",
    "        norm_off = 0.0\n",
    "        norm_sca = 1.0\n",
    "    else:\n",
    "        raise ValueError(f'Unknown normalization type: {NORMALIZE_TYPE}')\n",
    "\n",
    "    data = (data - norm_off) / norm_sca\n",
    "\n",
    "    return data, norm_off, norm_sca\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    seqs_batch_n = len(data) - seq_len - 1\n",
    "    sequences = torch.empty((seqs_batch_n, seq_len, 1))\n",
    "    targets = torch.empty((seqs_batch_n, 1))\n",
    "    for i in range(seqs_batch_n):\n",
    "        sequences[i] = data[i:i + seq_len].view(seq_len, 1)\n",
    "        targets[i] = data[i + seq_len + 1]\n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1690606128389,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "-Fj4RCsmAuMM"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layers_n, dropout_p):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=layers_n, dropout=dropout_p if layers_n > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(1), self.lstm.hidden_size, device=x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(1), self.lstm.hidden_size, device=x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out.select(1, -1))  # Select the last element from the seq_len dimension and apply dropout\n",
    "        out = self.linear(out)\n",
    "        out = out.squeeze(-1)  # Squeeze the last dimension if your output size is 1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1690606128389,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "amQauzheAuMM"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sine wave parameters for our fictitious market history\n",
    "TESTDATA_MIN_VAL = 1.0\n",
    "TESTDATA_MAX_VAL = 2.0\n",
    "TESTDATA_FREQUENCY = 1.0 / 100.0\n",
    "\n",
    "# How many minutes of history do we have?\n",
    "TESTDATA_SAMPLES_MINS_N = 60 * 24 * 30 * 3  # 3 months\n",
    "# Convert the minutes to samples\n",
    "TESTDATA_SAMPLES_N = TESTDATA_SAMPLES_MINS_N // SAMPLE_SIZE_MINS\n",
    "\n",
    "# Sine wave parameters for our fictitious market history\n",
    "TRAINDATA_MIN_VAL = 1.0\n",
    "TRAINDATA_MAX_VAL = 2.0\n",
    "TRAINDATA_FREQUENCY = 1.0 / 100.0\n",
    "\n",
    "# How many minutes of history do we have?\n",
    "TRAINDATA_SAMPLES_MINS_N = 60 * 24 * 30 * 6  # 6 months\n",
    "# Convert the minutes to samples\n",
    "TRAINDATA_SAMPLES_N = TRAINDATA_SAMPLES_MINS_N // SAMPLE_SIZE_MINS\n",
    "\n",
    "train_prices = generate_sine_wave(TRAINDATA_SAMPLES_N, TRAINDATA_FREQUENCY, TRAINDATA_MIN_VAL, TRAINDATA_MAX_VAL)\n",
    "test_prices = generate_sine_wave(TESTDATA_SAMPLES_N, TESTDATA_FREQUENCY, TESTDATA_MIN_VAL, TESTDATA_MAX_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4187,
     "status": "ok",
     "timestamp": 1690606132572,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "QzAO8gNkAuMN"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# Setup data for training\n",
    "train_input_data, train_mean, train_std = make_input_data(train_prices)\n",
    "train_seqs, train_targets = create_sequences(train_input_data, LSTM_SEQUENCE_LENGTH)\n",
    "train_seqs = train_seqs.to(device)\n",
    "train_targets = train_targets.to(device)\n",
    "\n",
    "# Setup data for testing\n",
    "test_input_data, _, _ = make_input_data(test_prices)\n",
    "test_seqs, test_targets = create_sequences(test_input_data, LSTM_SEQUENCE_LENGTH)\n",
    "test_seqs = test_seqs.to(device)\n",
    "test_targets = test_targets.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1690606132573,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "k4VuqY4cAuMN"
   },
   "outputs": [],
   "source": [
    "# use the Xavier scaling for initialization. Very effective in this case.\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    elif type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1690606132573,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "hxlZ02zMCTe_"
   },
   "outputs": [],
   "source": [
    "# A class to track and plot losses during training\n",
    "# We track training and testing losses separately and plot in the same chart\n",
    "class LossTracker:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.test_losses = []\n",
    "        self.min_test_loss = 1e10\n",
    "        self.min_test_loss_epoch = 0\n",
    "\n",
    "    def add_loss(self, train_loss, test_loss):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.test_losses.append(test_loss)\n",
    "        if test_loss < self.min_test_loss:\n",
    "            self.min_test_loss = test_loss\n",
    "            self.min_test_loss_epoch = len(self.test_losses)\n",
    "\n",
    "    def plot_losses(self, ax):\n",
    "        ax.plot(self.train_losses, label='Train Loss')\n",
    "        ax.plot(self.test_losses, label='Test Loss')\n",
    "        ax.set_title('Loss History')\n",
    "        ax.legend()\n",
    "\n",
    "        # Set y-axis to logarithmic scale for loss history\n",
    "        ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 7463,
     "status": "error",
     "timestamp": 1690606140032,
     "user": {
      "displayName": "Davide Pasca",
      "userId": "15895349759666062266"
     },
     "user_tz": -540
    },
    "id": "44oRt7RtAuMN",
    "outputId": "61a24707-2013-484e-e66f-e76567dd77b3"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8921ce4cdd57>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_seqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mtest_preds_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenormalizeVal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f962ff120a15>\u001b[0m in \u001b[0;36mdenormalizeVal\u001b[0;34m(val)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdenormalizeVal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mNORMALIZE_SCALE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_sine_wave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamplesN\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminVal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxVal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NORMALIZE_SCALE' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# Initialize the model\n",
    "net = Network(LSTM_INPUT_SIZE, LSTM_HIDDEN_SIZE, LSTM_LAYERS_N, DROPOUT_P)\n",
    "net.apply(init_weights)\n",
    "net = net.to(device)\n",
    "\n",
    "# Specify loss function and optimizer\n",
    "#optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE_ADAM, weight_decay=L2_WEIGHT_DECAY)\n",
    "#optimizer = optim.Adagrad(net.parameters(), lr=LEARNING_RATE_ADAGRAD, weight_decay=L2_WEIGHT_DECAY)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE_SGD, momentum=0.9, weight_decay=L2_WEIGHT_DECAY)\n",
    "#optimizer = optim.RMSprop(net.parameters(), lr=LEARNING_RATE_RMS_PROP, weight_decay=L2_WEIGHT_DECAY)\n",
    "#optimizer = optim.RAdam(net.parameters(), lr=LEARNING_RATE_RADAM, weight_decay=L2_WEIGHT_DECAY)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE_ADAMW, weight_decay=L2_WEIGHT_DECAY)\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.9)\n",
    "\n",
    "#loss_func = nn.L1Loss(reduction='mean')\n",
    "loss_func = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Track the loss history for both training and testing\n",
    "loss_track = LossTracker()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS_N):\n",
    "    train_preds = net(train_seqs)\n",
    "    train_loss = loss_func(train_preds, train_targets.squeeze(-1))\n",
    "\n",
    "    use_loss = train_loss\n",
    "    if L1_REGULARIZATION > 0.0:\n",
    "        l1_reg = 0.0\n",
    "        for param in net.parameters():\n",
    "            l1_reg += torch.norm(param, 1)\n",
    "        use_loss += L1_REGULARIZATION * l1_reg\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    use_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Step the scheduler\n",
    "    #scheduler.step()\n",
    "\n",
    "    isFirstOrLastEpoch = (epoch == 0 or epoch == (EPOCHS_N - 1))\n",
    "\n",
    "    if isFirstOrLastEpoch or (epoch % 10) == 0:\n",
    "        net.eval()  # Switch to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            test_preds = net(test_seqs)\n",
    "            test_loss = loss_func(test_preds, test_targets.squeeze(-1))\n",
    "\n",
    "        loss_track.add_loss(train_loss.item(), test_loss.item())\n",
    "        net.train()  # Switch back to training mode\n",
    "\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        net.eval()  # Switch to evaluation mode\n",
    "\n",
    "        test_preds_vec = []\n",
    "        for i in range(len(test_seqs)):\n",
    "            sequence = test_seqs[i].unsqueeze(0)\n",
    "            pred = net(sequence).item()\n",
    "            test_preds_vec.append(denormalizeVal(pred))\n",
    "\n",
    "        test_preds = torch.tensor(test_preds_vec)\n",
    "        test_preds = test_preds * train_std + train_mean\n",
    "\n",
    "        # Clear previous plots\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Create a figure\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "        # Plot the targets and predictions\n",
    "        axs[0].plot(test_targets.cpu().numpy(), label='True')\n",
    "        axs[0].plot(test_preds.cpu().numpy(), label='Predicted')\n",
    "        axs[0].set_title('Test Targets & Predictions')\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Plot the loss history\n",
    "        loss_track.plot_losses(axs[1])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS_N}], Train Loss: {train_loss.item()}, Test Loss: {test_loss.item()}\")\n",
    "\n",
    "        net.train()  # Switch back to training mode\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
